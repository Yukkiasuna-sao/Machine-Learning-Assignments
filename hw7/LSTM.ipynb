{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Concatenate text files to create a corpus of Russell’s writings\n",
    "meragefiledir = os.getcwd()+'/corpus'\n",
    "filenames = os.listdir(meragefiledir)\n",
    "f = open('corpus.txt','w')\n",
    "\n",
    "for file in filenames:  \n",
    "    filepath = meragefiledir+'/'+file\n",
    "    for line in open(filepath):\n",
    "        line = line.strip()\n",
    "        f.writelines(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "with open('corpus.txt','r',encoding='utf-8') as f:\n",
    "    sequence = f.read()\n",
    "c_dict = collections.Counter(sequence)\n",
    "# print(c_dict)\n",
    "chara_num = len(c_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L': 0.4595642712050536, 'E': 0.40152224035719286, 'C': 0.3531215849100996, 'T': 0.12807467406584705, 'U': 0.6840980825066243, 'R': 0.44389725986958994, ' ': 0.14050157811534425, 'I': 0.38873318348609665, 'N': 0.5682992521762926, 'D': 0.46666321271840816, 'S': 0.01821274710163856, 'P': 0.2889389003495424, 'h': 0.25517340577235414, 'i': 0.11969930288957975, 'l': 0.09424987291012799, 'o': 0.6261361154205785, 's': 0.05957109677799177, 'p': 0.3306379118349283, 'y': 0.7759468673125288, ',': 0.02813659927087675, 'f': 0.3412908437279376, 'r': 0.0052285421991598025, 'm': 0.8513711452844243, 't': 0.6007350401168765, 'e': 0.37651125907273875, 'a': 0.39379961044493905, 'd': 0.35012645812569954, 'g': 0.8809486649144623, 'c': 0.8351768513208411, 'n': 0.7881791935496637, 'v': 0.782601513726547, 'w': 0.5321377082954493, 'u': 0.6929800185318469, 'b': 0.5301564486444602, '.': 0.2551268608860393, '-': 0.33085657332672824, ';': 0.5326724201249082, 'q': 0.3459834999306757, 'A': 0.15993834043637456, 'x': 0.30976237370547655, 'k': 0.17415719352975811, 'K': 0.9276819495285636, 'H': 0.8631073298401966, 'W': 0.4309043938587137, 'J': 0.39115190008806133, 'M': 0.5810187695935033, 'B': 0.6751396084299061, '\"': 0.4276428330627615, 'G': 0.579061664022706, ':': 0.17821042604669157, 'F': 0.16988100240530124, 'ï': 0.44639782950909856, '_': 0.1103786206100752, \"'\": 0.40969889492512734, 'O': 0.5316392993396986, 'j': 0.5309352107242816, '?': 0.3466390565169919, '[': 0.40556991581951873, '4': 0.6462878326311856, ']': 0.39442074100091995, '\\xa0': 0.5442131359438007, '3': 0.10566936161439422, '2': 0.20114170625413463, 'â': 0.40499518482493824, 'æ': 0.6095670874171935, 'z': 0.9277738754060232, '5': 0.2250931843647519, 'Y': 0.5147944492071216, '1': 0.7370190784360854, '(': 0.10481179912181793, ')': 0.7361831743158879, '6': 0.8899698978380695, '8': 0.5795623555042256, '7': 0.6122239872966312, '9': 0.6337269676092332, '§': 0.6775032647358297, '0': 0.21251980966404815, 'V': 0.6892674901748274, 'X': 0.5106823496160677, '!': 0.07649598920144973, 'α': 0.8356908904228889, 'β': 0.7972367602304774, 'γ': 0.8574466541446133, 'é': 0.5341048708160058, 'Z': 0.33265703005519454, '/': 0.6594382447854817, 'Q': 0.2675043475817116, '|': 0.28509638452404185, '>': 0.8042168702382995, 'τ': 0.53032225698819, 'ὸ': 0.27575431068552736, 'π': 0.9502019710354458, 'ό': 0.646806239963717, 'σ': 0.23417858529034874, 'ο': 0.5969605906602754, 'ν': 0.7531533139864289, 'η': 0.8215552346255182, 'λ': 0.4179511801038962, 'ί': 0.8214059831442256, 'κ': 0.38042106469857195, 'ὴ': 0.6946967381918329, 'φ': 0.8899539295878839, 'ι': 0.26701759272528736, 'ρ': 0.20729875215758198, 'ή': 0.9550433345598998, '=': 0.7933425332507174, 'è': 0.859897130550818, 'ë': 0.6662829108075894, 'ü': 0.5040213966048921, 'É': 0.6467861907232588, '′': 0.4892636933356499, '″': 0.5156753481196982, '·': 0.0004592289779212777, '+': 0.8805354735444256, 'ŭ': 0.8872889595724238, '*': 0.45879532570809867, 'œ': 0.827982992684578, 'Æ': 0.970871889857527, 'ô': 0.7934456128349271, 'θ': 0.4910074957906617, '{': 0.6134266127123108, '}': 0.862333314096105, 'î': 0.4488506992296799, 'ö': 0.8958253790643015}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rand_list = np.random.random_sample((chara_num,))\n",
    "new_dict = dict(zip(c_dict.keys(), rand_list))\n",
    "print(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L': 0, 'E': 1, 'C': 2, 'T': 3, 'U': 4, 'R': 5, ' ': 6, 'I': 7, 'N': 8, 'D': 9, 'S': 10, 'P': 11, 'h': 12, 'i': 13, 'l': 14, 'o': 15, 's': 16, 'p': 17, 'y': 18, ',': 19, 'f': 20, 'r': 21, 'm': 22, 't': 23, 'e': 24, 'a': 25, 'd': 26, 'g': 27, 'c': 28, 'n': 29, 'v': 30, 'w': 31, 'u': 32, 'b': 33, '.': 34, '-': 35, ';': 36, 'q': 37, 'A': 38, 'x': 39, 'k': 40, 'K': 41, 'H': 42, 'W': 43, 'J': 44, 'M': 45, 'B': 46, '\"': 47, 'G': 48, ':': 49, 'F': 50, 'ï': 51, '_': 52, \"'\": 53, 'O': 54, 'j': 55, '?': 56, '[': 57, '4': 58, ']': 59, '\\xa0': 60, '3': 61, '2': 62, 'â': 63, 'æ': 64, 'z': 65, '5': 66, 'Y': 67, '1': 68, '(': 69, ')': 70, '6': 71, '8': 72, '7': 73, '9': 74, '§': 75, '0': 76, 'V': 77, 'X': 78, '!': 79, 'α': 80, 'β': 81, 'γ': 82, 'é': 83, 'Z': 84, '/': 85, 'Q': 86, '|': 87, '>': 88, 'τ': 89, 'ὸ': 90, 'π': 91, 'ό': 92, 'σ': 93, 'ο': 94, 'ν': 95, 'η': 96, 'λ': 97, 'ί': 98, 'κ': 99, 'ὴ': 100, 'φ': 101, 'ι': 102, 'ρ': 103, 'ή': 104, '=': 105, 'è': 106, 'ë': 107, 'ü': 108, 'É': 109, '′': 110, '″': 111, '·': 112, '+': 113, 'ŭ': 114, '*': 115, 'œ': 116, 'Æ': 117, 'ô': 118, 'θ': 119, '{': 120, '}': 121, 'î': 122, 'ö': 123}\n"
     ]
    }
   ],
   "source": [
    "chara_range = list(range(0, chara_num))\n",
    "index_dict = dict(zip(c_dict.keys(), chara_range))\n",
    "print(index_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting inputs and outputs to LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 100\n",
    "i = 0\n",
    "X = []\n",
    "Y = []\n",
    "while(i < (len(sequence)-window)):\n",
    "    in_tmp_list=[]\n",
    "    out_tmp_array = np.zeros((chara_num,), dtype=int)\n",
    "    inp = sequence[i : (i+window-1)]\n",
    "    outp = sequence[i+window-1]\n",
    "    for j in inp:\n",
    "        in_tmp_list.append(new_dict[j])\n",
    "    out_tmp_array[index_dict[outp]] += 1\n",
    "    X.append(in_tmp_list)\n",
    "    Y.append(out_tmp_array)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45956427 0.40152224 0.35312158 ... 0.37651126 0.78260151 0.37651126]\n",
      " [0.40152224 0.35312158 0.12807467 ... 0.78260151 0.37651126 0.35012646]\n",
      " [0.35312158 0.12807467 0.68409808 ... 0.37651126 0.35012646 0.14050158]\n",
      " ...\n",
      " [0.14050158 0.00522854 0.37651126 ... 0.60073504 0.14050158 0.88094866]\n",
      " [0.00522854 0.37651126 0.78817919 ... 0.14050158 0.88094866 0.62613612]\n",
      " [0.37651126 0.78817919 0.35012646 ... 0.88094866 0.62613612 0.62613612]]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# X is the inputs to LSTM, Y is the outputs\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(1044062, 99, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1044062/1044062 [==============================] - 730s 699us/step - loss: 2.7341 - acc: 0.2562\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.73405, saving model to weights-improvement-01-2.7341.hdf5\n",
      "Epoch 2/20\n",
      "1044062/1044062 [==============================] - 648s 621us/step - loss: 2.7110 - acc: 0.2591\n",
      "\n",
      "Epoch 00002: loss improved from 2.73405 to 2.71095, saving model to weights-improvement-02-2.7110.hdf5\n",
      "Epoch 3/20\n",
      "1044062/1044062 [==============================] - 649s 621us/step - loss: 2.7100 - acc: 0.2591\n",
      "\n",
      "Epoch 00003: loss improved from 2.71095 to 2.71001, saving model to weights-improvement-03-2.7100.hdf5\n",
      "Epoch 4/20\n",
      "1044062/1044062 [==============================] - 645s 618us/step - loss: 2.7092 - acc: 0.2591\n",
      "\n",
      "Epoch 00004: loss improved from 2.71001 to 2.70923, saving model to weights-improvement-04-2.7092.hdf5\n",
      "Epoch 5/20\n",
      "1044062/1044062 [==============================] - 657s 629us/step - loss: 2.7085 - acc: 0.2591\n",
      "\n",
      "Epoch 00005: loss improved from 2.70923 to 2.70853, saving model to weights-improvement-05-2.7085.hdf5\n",
      "Epoch 6/20\n",
      "1044062/1044062 [==============================] - 634s 608us/step - loss: 2.7080 - acc: 0.2591\n",
      "\n",
      "Epoch 00006: loss improved from 2.70853 to 2.70800, saving model to weights-improvement-06-2.7080.hdf5\n",
      "Epoch 7/20\n",
      "1044062/1044062 [==============================] - 635s 608us/step - loss: 2.7078 - acc: 0.2591\n",
      "\n",
      "Epoch 00007: loss improved from 2.70800 to 2.70775, saving model to weights-improvement-07-2.7078.hdf5\n",
      "Epoch 8/20\n",
      "1044062/1044062 [==============================] - 635s 608us/step - loss: 2.7075 - acc: 0.2591\n",
      "\n",
      "Epoch 00008: loss improved from 2.70775 to 2.70747, saving model to weights-improvement-08-2.7075.hdf5\n",
      "Epoch 9/20\n",
      "1044062/1044062 [==============================] - 634s 607us/step - loss: 2.7071 - acc: 0.2591\n",
      "\n",
      "Epoch 00009: loss improved from 2.70747 to 2.70711, saving model to weights-improvement-09-2.7071.hdf5\n",
      "Epoch 10/20\n",
      "1044062/1044062 [==============================] - 636s 609us/step - loss: 2.7067 - acc: 0.2591\n",
      "\n",
      "Epoch 00010: loss improved from 2.70711 to 2.70666, saving model to weights-improvement-10-2.7067.hdf5\n",
      "Epoch 11/20\n",
      "1044062/1044062 [==============================] - 625s 599us/step - loss: 2.7065 - acc: 0.2590\n",
      "\n",
      "Epoch 00011: loss improved from 2.70666 to 2.70648, saving model to weights-improvement-11-2.7065.hdf5\n",
      "Epoch 12/20\n",
      "1044062/1044062 [==============================] - 623s 596us/step - loss: 2.7063 - acc: 0.2590\n",
      "\n",
      "Epoch 00012: loss improved from 2.70648 to 2.70631, saving model to weights-improvement-12-2.7063.hdf5\n",
      "Epoch 13/20\n",
      "1044062/1044062 [==============================] - 623s 597us/step - loss: 2.7061 - acc: 0.2590\n",
      "\n",
      "Epoch 00013: loss improved from 2.70631 to 2.70608, saving model to weights-improvement-13-2.7061.hdf5\n",
      "Epoch 14/20\n",
      "1044062/1044062 [==============================] - 625s 598us/step - loss: 2.7059 - acc: 0.2591\n",
      "\n",
      "Epoch 00014: loss improved from 2.70608 to 2.70594, saving model to weights-improvement-14-2.7059.hdf5\n",
      "Epoch 15/20\n",
      "1044062/1044062 [==============================] - 632s 606us/step - loss: 2.7058 - acc: 0.2590\n",
      "\n",
      "Epoch 00015: loss improved from 2.70594 to 2.70576, saving model to weights-improvement-15-2.7058.hdf5\n",
      "Epoch 16/20\n",
      "1044062/1044062 [==============================] - 658s 630us/step - loss: 2.7056 - acc: 0.2590\n",
      "\n",
      "Epoch 00016: loss improved from 2.70576 to 2.70565, saving model to weights-improvement-16-2.7056.hdf5\n",
      "Epoch 17/20\n",
      "1044062/1044062 [==============================] - 1042s 998us/step - loss: 2.7055 - acc: 0.2590\n",
      "\n",
      "Epoch 00017: loss improved from 2.70565 to 2.70549, saving model to weights-improvement-17-2.7055.hdf5\n",
      "Epoch 18/20\n",
      "1044062/1044062 [==============================] - 1045s 1ms/step - loss: 2.7053 - acc: 0.2590\n",
      "\n",
      "Epoch 00018: loss improved from 2.70549 to 2.70532, saving model to weights-improvement-18-2.7053.hdf5\n",
      "Epoch 19/20\n",
      "1044062/1044062 [==============================] - 1043s 999us/step - loss: 2.7053 - acc: 0.2590\n",
      "\n",
      "Epoch 00019: loss did not improve from 2.70532\n",
      "Epoch 20/20\n",
      "1044062/1044062 [==============================] - 1111s 1ms/step - loss: 2.7052 - acc: 0.2590\n",
      "\n",
      "Epoch 00020: loss improved from 2.70532 to 2.70519, saving model to weights-improvement-20-2.7052.hdf5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 124)               62496     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 124)               0         \n",
      "=================================================================\n",
      "Total params: 62,496\n",
      "Trainable params: 62,496\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import h5py\n",
    "# build LSTM model\n",
    "model=Sequential()\n",
    "model.add(LSTM(units=400, input_shape=(X.shape[1], X.shape[2]), dropout=0.2))\n",
    "model.add(Dense(chara_num))\n",
    "model.add(Activation('softmax'))\n",
    "# compile: loss, optimizer, metrics\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# define the checkpoint\n",
    "filepath='weights-improvement-{epoch:02d}-{loss:.4f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(X, Y, epochs=20, batch_size=1000, callbacks=callbacks_list)\n",
    "print(model.summary())\n",
    "# loss, accuracy = model.evaluate(X, Y)\n",
    "# print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = 'weights-improvement-20-2.7052.hdf5'\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate characters\n",
    "int_to_char=dict(zip(index_dict.values(),index_dict.keys()))\n",
    "dataX = 'There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.'\n",
    "pattern = dataX[0:99]\n",
    "test_data=[]\n",
    "for x in pattern:\n",
    "    test_data.append([index_dict[x]])\n",
    "test_data=np.array(test_data)\n",
    "test_data=test_data.reshape(1,99,1)\n",
    "for i in range(1000):\n",
    "    X=test_data[0][i:]\n",
    "    X=X.reshape(1,X.shape[0],1)\n",
    "    prediction = model.predict(X, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "#     result = int_to_char[index]\n",
    "    test_data = test_data.reshape(99+i,1)\n",
    "    test_data=np.vstack((test_data, [index]))\n",
    "    test_data = test_data.reshape(1,test_data.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are those who take mental phenomena naively, just as they would physical phenomena. This schole ondyrose that the  on  of thesenises of the penaril sore te the contrady the pone al te be seal thans  of the pase of the onte that of the one andethed,ef enes to the corter of and the past of a  of erestr bot ve cases this is there os a  and there ar  of a  one cins in this peried the  ef the pense derire to the pase and thating to things which is core not to this,ether, thin whateis the pans of senind the pensende  ats and to fon  oth rone  fet re song not benser tratil  ondtinct, the  one os cencar  ase tho gerief  of this is the pan  andwhe onter an  and that the  one of eur pense detarns tiuld that the  are on  of the pase of the pan  of this werl is the pase on the condtituet and the pases on tise ofasper to the sase of tiuss ar  ot ic not bontention of this,indtire  and the  arter the pensesing and the ober to the pase ofether which ale serertirle the cord  ant al inrerencent of these is nn esected the perilion that the  ares andes the ponn and this intervectual the relation\n"
     ]
    }
   ],
   "source": [
    "gen=''\n",
    "for x in test_data[0]:\n",
    "    gen=gen+int_to_char[x[0]]\n",
    "print(pattern, gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
