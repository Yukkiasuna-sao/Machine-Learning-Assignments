{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#load train and test data\n",
    "train_data   = []\n",
    "test_data = []\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "\n",
    "with open(\"train_set.txt\") as f:  \n",
    "        for line in f:\n",
    "            tokens = line.strip().split(' ')  \n",
    "            train_data.append([float(tk) for tk in tokens[:-1]])  \n",
    "            train_labels.append(int(tokens[-1]))  \n",
    "x_train = np.array(train_data)\n",
    "y_train = np.array(train_labels)\n",
    "\n",
    "with open(\"test_set.txt\") as ff:  \n",
    "        for line in ff:\n",
    "            tokens = line.strip().split(' ')  \n",
    "            test_data.append([float(tk) for tk in tokens[:-1]])  \n",
    "            test_labels.append(int(tokens[-1])) \n",
    "x_test = np.array(test_data)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimal test error with Manhattan Distance is 0.11\n",
      "With Manhattan Distance, the optimal number of neighbors k in KNN is 6\n"
     ]
    }
   ],
   "source": [
    "#Replace the Euclidean metric with Manhattan Distance, Chebyshev Distance and Mahalanobis Distance\n",
    "\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "k_range = np.arange(1, 197, 5)\n",
    "#print(k_range)\n",
    "\n",
    "#i.Manhattan Distance\n",
    "test_error_manhattan = []\n",
    "for k in k_range:\n",
    "   #distance becomes Manhattan Distance when p = 1\n",
    "   knn = KNeighborsClassifier(n_neighbors = k, p=1, metric='minkowski')\n",
    "   knn.fit(x_train, y_train)\n",
    "   test_error_manhattan.append(1 - accuracy_score(y_test, knn.predict(x_test)))\n",
    "optimal_k_manhattan = k_range[test_error_manhattan.index(min(test_error_manhattan))]\n",
    "print('The minimal test error with Manhattan Distance is %.2f' % min(test_error_manhattan))\n",
    "print('With Manhattan Distance, the optimal number of neighbors k in KNN is %d' % optimal_k_manhattan)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(k_range, test_error, label='Test Errors')\n",
    "# # plt.vlines(optimal_k, plt.ylim()[0], np.max(test_error), linestyles = 'dashed', label='Optimal k on test')\n",
    "# plt.title('Test errors in terms of k (KNN with Manhattan Distance)')\n",
    "# plt.xlabel('Number of Neighbors - k')\n",
    "# plt.ylabel('Error rate')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.25892541  1.58489319  1.99526231  2.51188643  3.16227766  3.98107171\n",
      "  5.01187234  6.30957344  7.94328235 10.        ]\n",
      "The best log10(p) is 0.6\n"
     ]
    }
   ],
   "source": [
    "#explore best log10(p) for minkowski\n",
    "from scipy.spatial import distance\n",
    "\n",
    "lg_p = np.arange(0, 1, 0.1) + 0.1\n",
    "p_range = pow(10,lg_p)\n",
    "print(p_range)\n",
    "test_error_minkowski = []\n",
    "\n",
    "for q in p_range:\n",
    "     knn = KNeighborsClassifier(n_neighbors = optimal_k_manhattan, p=q, metric='minkowski')\n",
    "     knn.fit(x_train, y_train)\n",
    "     test_error_minkowski.append(1 - accuracy_score(y_test, knn.predict(x_test)))\n",
    "     i = test_error_minkowski.index(min(test_error_minkowski))\n",
    "    \n",
    "best_lg_p = lg_p[i]\n",
    "print('The best log10(p) is %.1f' % best_lg_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Chebyshev Distance, the optimal number of neighbors k in KNN is 16\n",
      "The minimal test error with Chebyshev Distance is 0.08\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import DistanceMetric\n",
    "#Chebyshev Distance when p->infinity\n",
    "test_error_chebyshev = []\n",
    "for k in k_range:\n",
    "   knn = KNeighborsClassifier(n_neighbors = k,metric='chebyshev')\n",
    "   knn.fit(x_train, y_train)\n",
    "   test_error_chebyshev.append(1 - knn.score(x_test, y_test))\n",
    "optimal_k = k_range[test_error_chebyshev.index(min(test_error_chebyshev))]\n",
    "print('With Chebyshev Distance, the optimal number of neighbors k in KNN is %d' % optimal_k)\n",
    "print('The minimal test error with Chebyshev Distance is %.2f' % min(test_error_chebyshev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With mahalanobis Distance, the optimal number of neighbors k in KNN is 1\n",
      "The minimal test error with Mahalanobis Distance is 0.17\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "test_error_mahalanobis = []\n",
    "for k in k_range:\n",
    "   knn = KNeighborsClassifier(n_neighbors = k,metric='mahalanobis', metric_params={'V':np.cov(np.array(x_train).T)})\n",
    "   knn.fit(x_train, y_train)\n",
    "   test_error_mahalanobis.append(1 - knn.score(x_test, y_test))\n",
    "optimal_k = k_range[test_error_mahalanobis.index(min(test_error_mahalanobis))]\n",
    "print('With mahalanobis Distance, the optimal number of neighbors k in KNN is %d' % optimal_k)\n",
    "print('The minimal test error with Mahalanobis Distance is %.2f' % min(test_error_mahalanobis))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0a3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
